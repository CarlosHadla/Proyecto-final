# -*- coding: utf-8 -*-
"""fhvhv_2019-2022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1If_Dw-Caah2ggdpcsUMq8wQ2QKVEk-Fx
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import os
import re

"""# Variables iniciales
Creamos las variables que van a contener los links de los datasets en la pagina de NYC OpenData y los nombres de los buckets del GCP donde cargar el archivo final.
"""

url_taxis = 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'
fhvhv_link_start = 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_'

"""# Distritos

Leemos el CSV que contiene todo el listado de distritos de Nueva York para poder complementar nuestra tabla inicial.
"""

boroughs_ny = pd.read_csv('/content/drive/MyDrive/Nati/Henry/PF - NYC Taxis/Sources/Taxis-NY/taxi+_zone_lookup.csv')

"""# Funciones

def get_parquet_links(url, link, rango): Función para obtener los enlaces de descarga de archivos
"""

def get_parquet_links(url, link, rango, rango_meses):
    response = requests.get(url)
    parquet_links = []
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', href=True)
        for i in rango:
          for j in rango_meses:
            link_str = str(link)
            parquet_links.extend([link['href'] for link in links if '.parquet' in link['href'] and link['href'].startswith(link_str + str(i) + '-' + str(j).zfill(2))])
    return parquet_links

"""def dataframe_create(parquet_link): Funcion que me da un dataframe del archivo."""

def dataframe_create(parquet_link):
  file_response = requests.get(parquet_link)
  if file_response.status_code == 200:
    with open('archivo.parquet', 'wb') as f:
      f.write(file_response.content)

    # Leer el archivo Parquet usando pandas
    df = pd.read_parquet('archivo.parquet')

    return df  # Devuelve el DataFrame leído desde el archivo Parquet
  else:
    print("No se pudo descargar el archivo Parquet")

def etl_groupby(dataframe):
  # Aquí puedes hacer lo que necesites con el DataFrame df
  df_unificado = pd.merge(dataframe, boroughs_ny, left_on='PULocationID', right_on='LocationID', how='left')
  df_unificado = df_unificado.drop(columns=['originating_base_num','dispatching_base_num','request_datetime','on_scene_datetime','dropoff_datetime','shared_request_flag','shared_match_flag','access_a_ride_flag','wav_request_flag','wav_match_flag', 'LocationID', 'Zone', 'service_zone'], axis=1)
  df_unificado.rename(columns={'Borough': 'pickup_borough'}, inplace=True)
  df_unificado = pd.merge(df_unificado, boroughs_ny, left_on='DOLocationID', right_on='LocationID', how='left')
  df_unificado.drop(columns=['LocationID', 'Zone','service_zone'],axis=1 ,inplace=True)
  df_unificado.rename(columns={'Borough': 'dropoff_borough'}, inplace=True)
  df_unificado['pickup_day'] = df_unificado['pickup_datetime'].dt.date
  df_unificado['pickup_hour'] = df_unificado['pickup_datetime'].dt.hour
  df_parcial = df_unificado.groupby(['hvfhs_license_num','pickup_day', 'pickup_hour', 'pickup_borough', 'dropoff_borough']).agg({
    'pickup_datetime': 'count',
    'trip_miles': 'sum',
    'trip_time': 'sum',
    'base_passenger_fare': 'sum',
    'tolls': 'sum',
    'bcf': 'sum',
    'sales_tax': 'sum',
    'congestion_surcharge': 'sum',
    'airport_fee': 'sum',
    'tips': 'sum',
    'driver_pay': 'sum'
    }).reset_index()

  df_parcial.columns = ['hvfhs_license_num','pickup_day', 'pickup_hour', 'pickup_borough', 'dropoff_borough', 'total_trips','trip_miles','trip_time', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax', 'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay']

  # Estandarizamos el tipo de dato
  columnas_int = ['total_trips', 'trip_time']
  columnas_float = ['trip_miles', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax', 'congestion_surcharge', 'airport_fee', 'tips', 'driver_pay']
  df_parcial[columnas_int] = df_parcial[columnas_int].astype(int)
  df_parcial[columnas_float] = df_parcial[columnas_float].astype(float)

  return df_parcial  # Devuelve el DataFrame parcial

"""# Proceso

1) Busco el link del archivo.parquet
"""

#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(2,3))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(3,4))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(4,5))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(5,6))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(6,7))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(7,8))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(8,9))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(9,10))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(10,11))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(11,12))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2019,2020), range(12,13))

#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(1,2))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(2,3))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(3,4))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(4,5))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(5,6))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(6,7))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(7,8))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(8,9))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(9,10))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(10,11))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(11,12))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2020,2021), range(12,13))

#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(1,2))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(2,3))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(3,4))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(4,5))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(5,6))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(6,7))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(7,8))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(8,9))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(9,10))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(10,11))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(11,12))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2021,2022), range(12,13))

#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(1,2))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(2,3))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(3,4))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(4,5))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(5,6))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(6,7))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(7,8))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(8,9))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(9,10))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(10,11))
#fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(11,12))
fhvhv_link = get_parquet_links(url_taxis, fhvhv_link_start, range(2022,2023), range(12,13))

"""2) Le hago el dataframe_create, que me crea el dataframe"""

fhvhv_df = dataframe_create(fhvhv_link[0])

total_filas = len(fhvhv_df)

# Calcular la cantidad de filas que deseas eliminar (la mitad inferior)
filas_a_seleccionar = total_filas // 2

# Eliminar la mitad inferior de los registros y guardar el resultado en un nuevo DataFrame
df_superior = fhvhv_df.drop(fhvhv_df.tail(filas_a_seleccionar).index)

df_inferior = fhvhv_df.tail(filas_a_seleccionar)

"""df_superior lo divido de vuelta en dos"""

total_filas = len(df_superior)

# Calcular la cantidad de filas que deseas eliminar (la mitad inferior)
filas_a_seleccionar = total_filas // 2

# Eliminar la mitad inferior de los registros y guardar el resultado en un nuevo DataFrame
df_superior_1 = df_superior.drop(df_superior.tail(filas_a_seleccionar).index)

df_inferior_1 = df_superior.tail(filas_a_seleccionar)

fhvhv_01_1_limpio = etl_groupby(df_superior_1)

fhvhv_01_2_limpio = etl_groupby(df_inferior_1)

fhvhv_01_limpio = pd.concat([fhvhv_01_1_limpio, fhvhv_01_2_limpio], ignore_index=True)

fhvhv_01_limpio.to_csv('/content/drive/MyDrive/Nati/Henry/PF - NYC Taxis/Sources/Taxis-NY/fhvhv_01_limpio.csv', index=False)

total_filas = len(df_inferior)

# Calcular la cantidad de filas que deseas eliminar (la mitad inferior)
filas_a_seleccionar = total_filas // 2

# Eliminar la mitad inferior de los registros y guardar el resultado en un nuevo DataFrame
df_superior_2 = df_inferior.drop(df_inferior.tail(filas_a_seleccionar).index)

df_inferior_2 = df_inferior.tail(filas_a_seleccionar)

fhvhv_02_1_limpio = etl_groupby(df_superior_2)

fhvhv_02_2_limpio = etl_groupby(df_inferior_2)

fhvhv_02_limpio = pd.concat([fhvhv_02_1_limpio, fhvhv_02_2_limpio], ignore_index=True)

fhvhv_02_limpio.to_csv('/content/drive/MyDrive/Nati/Henry/PF - NYC Taxis/Sources/Taxis-NY/fhvhv_02_limpio.csv', index=False)

fhvhv_parcial = pd.concat([fhvhv_01_limpio, fhvhv_02_limpio], ignore_index=True)

"""6) Me traigo el df_final y le concateno el nuevo df, si no existe el archivo lo creo."""

nombre_archivo = '/content/drive/MyDrive/Nati/Henry/PF - NYC Taxis/Sources/Taxis-NY/fhvhv_completo.csv'

# Verificar si el archivo 'fhvhv_complete.csv' existe en el directorio actual
if os.path.isfile(nombre_archivo):
    # Si el archivo existe, cargarlo en una variable y concatenarle 'df_parcial'
    df_final = pd.read_csv(nombre_archivo)
    fhvhv_completo = pd.concat([df_final, fhvhv_parcial], ignore_index=True)
    # Guardar el DataFrame resultante en un archivo CSV
    fhvhv_completo.to_csv(nombre_archivo, index=False)
else:
    # Si el archivo no existe, crear una nueva variable 'fhvhv_complete' con el contenido de 'df_parcial'
    fhvhv_completo = fhvhv_parcial.copy()
    # Guardar 'df_parcial' como un nuevo archivo CSV llamado 'fhvhv_complete.csv'
    fhvhv_completo.to_csv(nombre_archivo, index=False)

fhvhv_completo