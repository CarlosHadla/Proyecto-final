# -*- coding: utf-8 -*-
"""Yellow_taxis_2014-2023.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D3xy6MoapNbwQued9gGVorxz30rm8_nV

Librerias
"""

from google.colab import drive
drive.mount('/content/drive')

import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import re

"""# Variables iniciales

Creamos las variables que van a contener los links de los datasets en la pagina de NYC OpenData y los nombres de los buckets del GCP donde cargar el archivo final.
"""

url_taxis = 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'
yellow_link = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_'

"""Leemos el CSV que contiene todo el listado de distritos de Nueva York para poder complementar nuestra tabla inicial."""

boroughs_ny = pd.read_csv('/content/drive/MyDrive/Nati/Henry/PF - NYC Taxis/Sources/Taxis-NY/taxi+_zone_lookup.csv')

"""# Funciones

def get_parquet_links(url, link, rango):
Función para obtener los enlaces de descarga de archivos

"""

def get_parquet_links(url, link, rango):
    response = requests.get(url)
    parquet_links = []
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', href=True)
        for i in rango:
            link_str = str(link)
            parquet_links.extend([link['href'] for link in links if '.parquet' in link['href'] and link['href'].startswith(link_str + str(i))])
    return parquet_links

"""def descarga_df(parquet_link): Funcion que me descarga el df desde el origen, con un pequeño filtro segun el año y mes que indica el nombre del archivo."""

def descarga_df(parquet_link):
  file_response = requests.get(parquet_link)
  if file_response.status_code == 200:
    with open('archivo.parquet', 'wb') as f:
      f.write(file_response.content)

    # Leer el archivo Parquet usando pandas
    df = pd.read_parquet('archivo.parquet')

    # Se extrae el mes y año de la info del archivo para luego poder filtrar el dataset
    if parquet_link.endswith('.parquet'):
      year_month = parquet_link.split('_')[-1].split('.')[0]  # Extraer el año y mes desde el nombre del archivo
      year_month_date = pd.to_datetime(year_month, format='%Y-%m')
      year = year_month_date.year
      month = year_month_date.month

    # Filtramos el dataset para que unicamente tenga la informacion del mes correcto
    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])
    df = df[(df['tpep_pickup_datetime'].dt.year == year) & (df['tpep_pickup_datetime'].dt.month == month)]
    return df

  else:
    print("No se pudo descargar el archivo Parquet")

"""def filtrar_df(df): Funcion que me realiza ciertos filtros al dataframe"""

def filtrar_df(df):
  # Rellenamos las columnas clave
  df['trip_distance'].fillna(0, inplace=True)
  df['fare_amount'].fillna(0, inplace=True)
  df['passenger_count'].fillna(1, inplace=True)

  # Realizamos filtros basicos para normalizar las columnas
  df = df[(df['trip_distance'] > 0) & (df['trip_distance'] <= 20)]
  df = df[(df['PULocationID'] != 264) & (df['DOLocationID'] != 265)]
  df = df[(df['fare_amount'] > 0) & (df['fare_amount'] < 100)]
  df['passenger_count'] = df['passenger_count'].replace(0, 1)
  df['mta_tax'] = 0.5

  # Convertir valores negativos a positivos en columnas numéricas
  columnas_numericas = df.select_dtypes(include='number')
  df[columnas_numericas.columns] = df[columnas_numericas.columns].abs()

  return df

"""def groupby_df(df): Funcion que me agrupa según el dia y hora de recogida del viaje, ademas de el punto de partida y el punto final del recorrido y por ultimo el tipo de pago que se realiza."""

def groupby_df(df):
  # Merge para saber los boroughs, tanto de pickup como de dropoff
  df_unificado = pd.merge(df, boroughs_ny, left_on='PULocationID', right_on='LocationID', how='left')
  df_unificado = df_unificado.drop(columns=['VendorID','tpep_dropoff_datetime','store_and_fwd_flag', 'LocationID', 'Zone', 'service_zone'], axis=1)
  df_unificado.rename(columns={'Borough': 'pickup_borough'}, inplace=True)
  df_unificado = pd.merge(df_unificado, boroughs_ny, left_on='DOLocationID', right_on='LocationID', how='left')
  df_unificado.drop(columns=['LocationID', 'Zone','service_zone'],axis=1 ,inplace=True)
  df_unificado.rename(columns={'Borough': 'dropoff_borough'}, inplace=True)

  # Agregamos la columna pickup_day y pickup_hour para agrupar
  df_unificado['pickup_day'] = df_unificado['tpep_pickup_datetime'].dt.date
  df_unificado['pickup_hour'] = df_unificado['tpep_pickup_datetime'].dt.hour

  df_unificado['pickup_day'] = pd.to_datetime(df_unificado['pickup_day']).dt.strftime('%Y-%m-%d')

  # Agrupamos segun el dia, hora, borough de pickup y de dropoff y la forma de pago
  df_parcial = df_unificado.groupby(['pickup_day', 'pickup_hour', 'pickup_borough', 'dropoff_borough','payment_type']).agg({
    'tpep_pickup_datetime': 'count',
    'passenger_count': 'sum',
    'trip_distance': 'sum',
    'fare_amount': 'sum',
    'extra': 'sum',
    'mta_tax': 'sum',
    'tip_amount': 'sum',
    'tolls_amount': 'sum',
    'improvement_surcharge': 'sum',
    'total_amount': 'sum',
    'congestion_surcharge': 'sum'
    }).reset_index()

  df_parcial['pickup_day'] = pd.to_datetime(df_parcial['pickup_day'])

  df_parcial.columns = ['pickup_day', 'pickup_hour', 'pickup_borough', 'dropoff_borough', 'payment_type', 'total_trips', 'passenger_count', 'total_distance', 'fare_amount', 'extra_hour', 'tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']

  # Estandarizamos el tipo de dato
  columnas_int = ['payment_type', 'total_trips', 'passenger_count']
  columnas_float = [ 'total_distance', 'fare_amount', 'extra_hour', 'tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']
  df_parcial[columnas_int] = df_parcial[columnas_int].astype(int)
  df_parcial[columnas_float] = df_parcial[columnas_float].astype(float)

  return df_parcial  # Devuelve el DataFrame leído desde el archivo Parquet

"""def concatenar_archivos(links): Funcion que concatena todos los dataframes que nos devuelve la funcion etl_dataset(parquet_link)"""

def concatenar_archivos(links):
  yellow_dfs = []

    for parquet_link in links:
        try:
            df_1 = descarga_df(parquet_link)
        except:
            print(f"Archivo {parquet_link} no pudo descargarse")
            continue

        df_2 = filtrar_df(df_1)
        df_parcial = groupby_df(df_2)
        yellow_dfs.append(df_parcial)
  # Concatenar todos los DataFrames fuera del bucle
  yellow_parcial = pd.concat(yellow_dfs).reset_index(drop=True)
  return yellow_parcial

"""def concatenar_dataframes(dataframes): Funcion que me une todos los datasets armados previamente."""

def concatenar_dataframes(dataframes):
  yellow_dfs = []

  for df in dataframes:
      yellow_dfs.append(df)

  # Concatenar todos los DataFrames fuera del bucle
  yellow_completo = pd.concat(yellow_dfs).reset_index(drop=True)
  return yellow_completo

"""# Yellow Taxis 2014-2023
Obtenemos los links de todos los archivos por año, en este caso decidimos separarlo primero por año ya que automatizarlo implicaba un costo de memoria RAM que supera el límite.
"""

yellow_link_2014 = get_parquet_links(url_taxis, yellow_link, range(2014,2015))
yellow_link_2015 = get_parquet_links(url_taxis, yellow_link, range(2015,2016))
yellow_link_2016 = get_parquet_links(url_taxis, yellow_link, range(2016,2017))
yellow_link_2017 = get_parquet_links(url_taxis, yellow_link, range(2017,2018))
yellow_link_2018 = get_parquet_links(url_taxis, yellow_link, range(2018,2019))
yellow_link_2019 = get_parquet_links(url_taxis, yellow_link, range(2019,2020))
yellow_link_2020 = get_parquet_links(url_taxis, yellow_link, range(2020,2021))
yellow_link_2021 = get_parquet_links(url_taxis, yellow_link, range(2021,2022))
yellow_link_2022 = get_parquet_links(url_taxis, yellow_link, range(2022,2023))
yellow_link_2023 = get_parquet_links(url_taxis, yellow_link, range(2023,2024))

yellow_link_2023 = get_parquet_links(url_taxis, yellow_link, range(2023,2024))

"""Hacemos el mismo procedimiento para cada año por separado, para evitar colapsar la memoria RAM, y descargando un archivo .csv de cada año por si se genera el colapso, no tener que volver a arrancar."""

yellow_2022 = concatenar_archivos(yellow_link_2022)

yellow_2022.to_csv('/content/drive/MyDrive/Nati/Henry/Yellow/yellow_2022.csv', index=False)

yellow_2021 = concatenar_archivos(yellow_link_2021)

yellow_2021.to_csv('/content/drive/MyDrive/Nati/Henry/Yellow/yellow_2021.csv', index=False)

yellow_2020 = concatenar_archivos(yellow_link_2020)

yellow_2020.to_csv('/content/drive/MyDrive/Nati/Henry/Yellow/yellow_2020.csv', index=False)

yellow_2019 = concatenar_archivos(yellow_link_2019)

yellow_2019.to_csv('/content/drive/MyDrive/Nati/Henry/Yellow/yellow_2019.csv', index=False)

yellow_2018 = concatenar_archivos(yellow_link_2018)

yellow_2018.to_csv('yellow_2018.csv', index=False)

yellow_2017 = concatenar_archivos(yellow_link_2017)

yellow_2017.to_csv('yellow_2017.csv', index=False)

yellow_2016 = concatenar_archivos(yellow_link_2016)

yellow_2016.to_csv('/content/drive/MyDrive/Nati/Henry/Yellow/yellow_2016.csv', index=False)

yellow_2015 = concatenar_archivos(yellow_link_2015)

yellow_2015.to_csv('/content/drive/MyDrive/Nati/Henry/Yellow/yellow_2015.csv', index=False)

yellow_2014 = concatenar_archivos(yellow_link_2014)

yellow_2014.to_csv('/content/drive/MyDrive/Nati/Henry/PF - NYC Taxis/Yellow/yellow_2014.csv', index=False)

yellow_2023 = concatenar_archivos(yellow_link_2023)

yellow_2023['pickup_day'].dt.month.unique()

yellow_2023.to_csv('/content/drive/MyDrive/Nati/Henry/Yellow/yellow_2023.csv', index=False)

"""Hacemos un listado de todos los dataframes que creamos y luego ejecutamos la funcion anterior.


"""

yellow_df = [yellow_2014, yellow_2015, yellow_2016, yellow_2017, yellow_2018, yellow_2019, yellow_2020, yellow_2021, yellow_2022, yellow_2023]

yellow_completo = concatenar_dataframes(yellow_df)

"""Modificamos el tipo de dato de la columna pickup_day a tipo str para que pueda ser transformado en un .parquet"""

yellow_completo['pickup_day'] = yellow_completo['pickup_day'].astype(str)

"""Transformamos en parquet el archivo final para poder subirlo al storage de la nube."""

yellow_completo.to_parquet('yellow_completo.parquet', index=False)





"""# EDA pequeño"""

yellow_2021 = descarga_df(yellow_link_2021[0])

# Rellenamos las columnas clave
yellow_2021['trip_distance'].fillna(0, inplace=True)
yellow_2021['fare_amount'].fillna(0, inplace=True)
yellow_2021['passenger_count'].fillna(1, inplace=True)

# Realizamos filtros basicos para normalizar las columnas
yellow_2021 = yellow_2021[(yellow_2021['trip_distance'] > 0) & (yellow_2021['trip_distance'] <= 20)]
yellow_2021 = yellow_2021[(yellow_2021['PULocationID'] != 264) & (yellow_2021['DOLocationID'] != 265)]
yellow_2021 = yellow_2021[yellow_2021['fare_amount'] > 0]
yellow_2021['passenger_count'] = yellow_2021['passenger_count'].replace(0, 1)
yellow_2021['mta_tax'] = 0.5

# Convertir valores negativos a positivos en columnas numéricas
columnas_numericas = yellow_2021.select_dtypes(include='number')
yellow_2021[columnas_numericas.columns] = yellow_2021[columnas_numericas.columns].abs()

yellow_2021.reset_index(drop=True, inplace=True)

import matplotlib.pyplot as plt

numeric_cols = yellow_2021.select_dtypes(include='number').columns
columns_to_drop = ['VendorID', 'passenger_count', 'payment_type', 'PULocationID', 'DOLocationID', 'RatecodeID']
numeric_cols = numeric_cols.drop(columns_to_drop, errors='ignore')

# Crear una figura y ejes para los boxplots
plt.figure(figsize=(10, 8))

# Crear un boxplot para cada columna numérica
plt.boxplot(yellow_2021[numeric_cols].values, vert=True, patch_artist=True)
plt.xticks(range(1, len(numeric_cols) + 1), numeric_cols, rotation=45)
plt.xlabel('Columns')
plt.ylabel('Values')
plt.title('Boxplots of Numeric Columns')

# Mostrar el gráfico
plt.tight_layout()
plt.show()

